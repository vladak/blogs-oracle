<html><body>
<head>
            <title>ZFS is going to save my laptop data next time | Oracle Czech techie's adventures Blog</title>
<meta name="publish_date" content="2008-04-03 15:24:04">
</head>
                                                                    <p>The flashback is still alive even weeks after: the day before my presentation at <a href="http://blogs.sun.com/vlad/entry/first_technical_colloquium_in_prague">FIRST Technical <br/>Colloquium in Prague</a> I brought my 2 years old laptop with the work-in-progress slides to the office.<br/>Since I wanted to finish the slides in the evening a <a href="http://hell.jedicoder.net/?p=39">live-upgrade</a><br/>process was fired off on the laptop to get fresh <a href="http://opensolaris.org/os/project/onnv/">Nevada</a><br/>version. (of course, to show off during the presentation ;))<br/><br/><br/><a href="http://www.sun.com/software/solaris/howtoguides/liveupgradehowto.jsp">LU</a> <br/>is very I/O intensive process and the red Ferrari notebooks tend to get _very_ hot. In the afternoon I<br/>noticed that the process failed. To my astonishment, the I/O operations started to fail.<br/>After couple of reboots (and <tt>zpool status</tt> / <tt>fmadm faulty</tt> commands) <br/>it was obvious that the disk cannot be trusted anymore. I was able to rescue some data<br/>from the ZFS pool which was spanning the biggest slice of the internal disk but not all data.<br/>(ZFS is not willing to get corrupted data out.) My slides were lost as well as other data.</p><p>After some time I stumbled upon <a href="http://research.sun.com/minds/2004-0624/">James Gosling's</a><br/><a href="http://blogs.sun.com/jag/entry/solaris_and_os_x"> blog entry about ZFS mirroring on laptop</a>.<br/>This get me started (or more precisely I was astonished and wondered how is it possible<br/>that this idea escaped me because at that time ZFS had been in Nevada for a long time)<br/>and I have discovered several similar and <a href="http://blogs.sun.com/erickustarz/entry/zfs_on_a_laptop">more</a><br/><a href="http://solarisdesktop.blogspot.com/2007/02/stick-to-zfs-or-laptop-with-mirrored.html">in-depth</a> <br/>blog entries about the topic.<br/><br/><br/>After some experiments with borrowed USB disk it was time to make it reality<br/>on a <a href="http://blogs.sun.com/vlad/resource/Maxtor_and_Ferrari_with_ZFS.jpg">new laptop</a>.</p><p>The process was a multi-step one:</p><br/><ol><li><br/>  First I had to extend the free slice #7 on the internal disk so it spans the remaining space<br/>  on the disk because it was trimmed after the experiments. In the end the slices look like this<br/>  in format(1) output:<pre>Part      Tag    Flag     Cylinders         Size            Blocks<br/>  0       root    wm       3 -  1277        9.77GB    (1275/0/0)   20482875<br/>  1 unassigned    wm    1278 -  2552        9.77GB    (1275/0/0)   20482875<br/>  2     backup    wm       0 - 19442      148.94GB    (19443/0/0) 312351795<br/>  3       swap    wu    2553 -  3124        4.38GB    (572/0/0)     9189180<br/>  4 unassigned    wu       0                0         (0/0/0)             0<br/>  5 unassigned    wu       0                0         (0/0/0)             0<br/>  6 unassigned    wu       0                0         (0/0/0)             0<br/>  7       home    wm    3125 - 19442      125.00GB    (16318/0/0) 262148670<br/>  8       boot    wu       0 -     0        7.84MB    (1/0/0)         16065<br/>  9 alternates    wu       1 -     2       15.69MB    (2/0/0)         32130</pre></li><li><br/>  Then the USB drive was connected to the system and recognized via format(1):<pre>AVAILABLE DISK SELECTIONS:<br/>       0. c0d0 <DEFAULT cyl 19443 alt 2 hd 255 sec 63><br/>          /pci@0,0/pci-ide@12/ide@0/cmdk@0,0<br/>       1. c5t0d0 <Maxtor-OneTouch-0125-149.05GB><br/>          /pci@0,0/pci1025,10a@13,2/storage@4/disk@0,0</pre></li><li><br/>  Live upgrade boot environment which was not active was deleted via ludelete(1M)<br/>  and the slice was commented out in <tt>/etc/vfstab</tt>. This was needed to make<br/>  zpool(1M) happy.</li><li><br/>  ZFS pool was created out of the slice on the internal disk (c0d0s7) and external<br/>  USB disk (c5t0d0). I had to force it cause zpool(1M) complained about the overlap<br/>  of c0d0s2 (slice spanning the whole disk) and c0d0s7:<pre># zpool create -f data mirror c0d0s7 c5t0d0</pre><br/>  For a while I have struggled with finding a name for the pool (everybody seems <br/>  either to stick to the 'tank' name or come up with some double-cool-stylish name<br/>  which I wanted to avoid because of the likely degradation of the excitement from that name)<br/>  but then chosen the ordinary <tt>data</tt> (it's what it is, after all).</li><li>I have verified that it is possible to disconnect the USB disk <br/>  and safely connect it while an I/O operation is in progress:<pre>root:moose:/data# mkfile 10g /data/test &<br/>[1] 10933<br/>root:moose:/data# zpool status<br/>  pool: data<br/> state: ONLINE<br/> scrub: none requested<br/>config:<br/><p style="text-indent:5em;"></p>NAME        STATE     READ WRITE CKSUM<br/><p style="text-indent:5em;"></p>data        ONLINE       0     0     0<br/><p style="text-indent:5em;"></p>  mirror    ONLINE       0     0     0<br/><p style="text-indent:5em;"></p>    c0d0s7  ONLINE       0     0     0<br/><p style="text-indent:5em;"></p>    c5t0d0  ONLINE       0     0     0<br/>errors: No known data errors</pre><br/>  It survived it without a hitch (okay, I had to wait for the zpool command to complete<br/>  a little bit longer due to the still ongoing I/O but that was it) and resynced<br/>  the contents automatically after the USB disk was reconnected:<pre>root:moose:/data# zpool status<br/>  pool: data<br/> state: DEGRADED<br/>status: One or more devices are faulted in response to persistent errors.<br/><p style="text-indent:5em;"></p>Sufficient replicas exist for the pool to continue functioning in a<br/><p style="text-indent:5em;"></p>degraded state.<br/>action: Replace the faulted device, or use 'zpool clear' to mark the device<br/><p style="text-indent:5em;"></p>repaired.<br/> scrub: resilver in progress for 0h0m, 3.22% done, 0h5m to go<br/>config:<br/><p style="text-indent:5em;"></p>NAME        STATE     READ WRITE CKSUM<br/><p style="text-indent:5em;"></p>data        DEGRADED     0     0     0<br/><p style="text-indent:5em;"></p>  mirror    DEGRADED     0     0     0<br/><p style="text-indent:5em;"></p>    c0d0s7  ONLINE       0     0     0<br/><p style="text-indent:5em;"></p>    c5t0d0  FAULTED      0     0     0  too many errors<br/>errors: No known data errors</pre><br/>  Also, with heavy I/O it is needed to mark the zpool as clear after the resilver completes <br/>  via <tt>zpool clear data</tt> because the USB drive is marked as faulty. Normally this will<br/>  not happen (unless the drive really failed) because I will be connecting and disconnecting<br/>  the drive only when powering on or shutting down the laptop, respectively.</li><li>After that I have used <br/><a href="http://blogs.sun.com/marks/">Mark Shellenbaum</a>'s <br/><a href="http://blogs.sun.com/marks/entry/zfs_delegated_administration">blog entry<br/>about ZFS delegated administration</a> (it was Mark who did <br/><a href="http://src.opensolaris.org/source/search?q=&defs=&refs=&path=&hist=%22ZFS+delegated+administration%22&project=%2Fonnv">the integration</a>)<br/><!--<br/>which was covered by<br/>PSARC/2006/465, currently the materials are not public) <br/>--><br/>and <a href="http://dlc.sun.com/osol/docs/content/ZFSADMIN/gbchv.html">ZFS Delegated Administration<br/>chapter</a> from <a href="http://dlc.sun.com/osol/docs/content/ZFSADMIN/p1.html">OpenSolaris<br/>ZFS Administration Guide</a> and created permissions set for my local user and assigned<br/>those permissions to the ZFS pool 'data' and the user:<pre>  # chmod A+user:vk:add_subdirectory:fd:allow /data<br/>  # zfs allow -s @major_perms clone,create,destroy,mount,snapshot data<br/>  # zfs allow -s @major_perms send,receive,share,rename,rollback,promote data<br/>  # zfs allow -s @major_props copies,compression,quota,reservation data<br/>  # zfs allow -s @major_props snapdir,sharenfs data<br/>  # zfs allow vk @major_perms,@major_props data</pre><br/>  All of the commands had to be done under root.</li><li>Now the user is able to create a home directory for himself:<pre>  &#36; zfs create data/vk</pre></li><li>Time to setup the environment of data sets and prepare it for data.<br/>  I have separated the data sets according to a 'service level'. Some data are very<br/>  important (e.g. presentations ;)) to me so I want them multiplied via the <a href="http://blogs.sun.com/bill/entry/ditto_blocks_the_amazing_tape">ditto blocks</a><br/>  mechanism so they are actually present 4 times in case of <tt>copies</tt> dataset<br/>  property set to 2. Also, documents are not usually accompanied by executable<br/>  code so the <tt>exec</tt> property was set to <tt>off</tt> which will prevent<br/>  running scripts or programs from that dataset.<br/><br/>  Some data are volatile and in high quantity so they do not need any additional<br/>  protection and it is good idea to<br/>  compress them with <a href="http://blogs.sun.com/ahl/entry/gzip_for_zfs_update">better <br/>  compression algorithm</a> to save some space. The following table summarizes the setup:<pre>   dataset             properties                       comment<br/> +-------------------+--------------------------------+------------------------+<br/> | data/vk/Documents | copies=2                       | presentations          |<br/> | data/vk/Saved     | compression=on exec=off        | stuff from web         |<br/> | data/vk/DVDs      | compression=gzip-8 exec=off    | Nevada ISOs for LU     |<br/> | data/vk/CRs       | compression=on copies=2        | precious source code ! |<br/> +-------------------+--------------------------------+------------------------+</pre><br/>  So the commands will be:<pre>  &#36; zfs create -o copies=2 data/vk/Documents<br/>  &#36; zfs create -o compression=gzip-3 -o exec=off data/vk/Saved<br/>  ...</pre></li><li><br/>  Now it is possible to migrate all the data, change home directory of the<br/>  user to <tt>/data/vk</tt> (e.g. via <tt>/usr/ucb/vipw</tt>) and relogin.</li><br/></ol><p>However, this is not the end of it but just beginning. There are many things to<br/>make the setup even better, to name a few: </p><ul><li>setup some sort of automatic snapshots for selected datasets<br/><br/>  The set of scripts and SMF service for doing ZFS snapshots and backup <br/>  (see <a href="http://blogs.sun.com/timf/entry/zfs_automatic_for_the_people">ZFS Automatic For The People</a><br/>  and related blog entries)<br/>  made by <a href="http://blogs.sun.com/timf/">Tim Foster</a> could be used for this task.</li><li>make <tt>zpool scrub</tt> run periodically<br/></li><li>detect failures of the disks<br/><br/>  This would be ideal to see in Gnome panel or Gnome system monitor.</li><li>setup off-site backup via <a href="http://opensolaris.org/os/community/security/projects/SSH/">SunSSH</a> + <tt>zfs send</tt><br/><br/>  This could be done using the hooks provided by Tim's scripts (see above). </li><li>Set quotas and reservations for some of the datasets.</li><li>Install ZFS scripts for Gnome nautilus so I will be able to browse, perform and destroy <br/>  snapshots in nautilus. Now which set of scripts to use ? <a href="http://blogs.sun.com/chrisg/entry/nautilus_meets_zfs_snapshots">Chris Gerhard's</a> or <a href="http://blogs.sun.com/timf/entry/zfs_on_your_desktop">Tim Foster's</a> ? Or should I<br/>  just wait for the <a href="http://www.opensolaris.org/os/project/jds/tasks/zfs_nautilus/">official<br/>  ZFS support for nautilus</a> to be integrated ?</li><li>Find how exactly will the recovery scenario (in case of laptop drive failure) will look like.<br/><br/>  To import the ZFS pool from the USB disk should suffice but during my experiments<br/>  I was not able to complete it successfully.</li></ul><p>With all the above the data should be safe from disk failure (after all disks are<br/>often called "spinning rust" so they are going to fail sooner or later) and also<br/>the event of loss of both laptop and USB disk.</p><p>Lastly, a philosophical thought: One of my colleagues considers hardware <br/>as a necessary (and very faulty) layer which is only needed to make it possible to<br/>express the ideas in software. This might seem extreme but come to think of it. <br/>ZFS is special in this sense - being a software which provides that bridge, it's core idea to <br/>isolate the hardware faults.</p>
</body></html>
